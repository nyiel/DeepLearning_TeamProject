{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6417582,"sourceType":"datasetVersion","datasetId":3701557}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================================\n# ğŸª´ Medicinal Leaf Classification - Flask-Ready Model\n# Classes: Mango, Neem, Guava, Lemon\n# Authors: (Ayuen, Malith, Deng, Biar & Akot)\n# ================================================================\n# This notebook trains a leaf classifier in two stages:\n#  1ï¸âƒ£ Base CNN from scratch\n#  2ï¸âƒ£ Transfer Learning using EfficientNetB7\n# The final model is saved for easy use in a Flask web app.\n# ================================================================\n\n# ------------------------------\n# 1. IMPORT LIBRARIES\n# ------------------------------\nimport os\nimport random\nfrom glob import glob\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json\nimport cv2\nimport albumentations as A\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nprint('âœ… TensorFlow version:', tf.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:38:13.172555Z","iopub.execute_input":"2025-11-02T18:38:13.173162Z","iopub.status.idle":"2025-11-02T18:38:13.178741Z","shell.execute_reply.started":"2025-11-02T18:38:13.173136Z","shell.execute_reply":"2025-11-02T18:38:13.178071Z"}},"outputs":[{"name":"stdout","text":"âœ… TensorFlow version: 2.18.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ------------------------------\n# 2. CONFIGURATION\n# ------------------------------\n# Adjust these paths for your environment (Kaggle or local)\nDATA_DIR = '/kaggle/input/indian-medicinal-leaves-dataset/Indian Medicinal Leaves Image Datasets/Medicinal Leaf dataset'\nWORK_DIR = '/kaggle/working/models'\n\n# Create working directory if not exists\nos.makedirs(WORK_DIR, exist_ok=True)\n\n# Image and training parameters\nIMG_SIZE = 224\nBATCH_SIZE = 16\nEPOCHS_BASE = 8\nEPOCHS_TRANSFER = 10\nRANDOM_SEED = 42\n\n# Target classes (4 specific plants)\nTARGET_CLASSES = ['Mango', 'Neem', 'Guava', 'Lemon']\n\n# Set global seeds for reproducibility\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:38:20.486604Z","iopub.execute_input":"2025-11-02T18:38:20.487124Z","iopub.status.idle":"2025-11-02T18:38:20.492041Z","shell.execute_reply.started":"2025-11-02T18:38:20.487099Z","shell.execute_reply":"2025-11-02T18:38:20.491431Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ------------------------------\n# 3. BUILD METADATA FROM FOLDERS\n# ------------------------------\ndef build_meta_from_folders(root_dir, extensions=('jpg', 'jpeg', 'png')):\n    \"\"\"\n    Scans dataset folders and returns a DataFrame with image file paths and labels.\n    \"\"\"\n    rows = []\n    for cls in sorted(os.listdir(root_dir)):\n        if cls not in TARGET_CLASSES:\n            continue  # Ignore unwanted classes\n        cls_path = os.path.join(root_dir, cls)\n        if not os.path.isdir(cls_path):\n            continue\n        for ext in extensions:\n            for fp in glob(os.path.join(cls_path, '**', f'*.{ext}'), recursive=True):\n                rows.append({'file_path': fp, 'label': cls})\n    return pd.DataFrame(rows)\n\nmeta = build_meta_from_folders(DATA_DIR)\nif meta.empty:\n    raise ValueError(f\"No images found for classes {TARGET_CLASSES}\")\n\nprint(f\"ğŸ“¸ Total images found: {len(meta)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:38:24.682134Z","iopub.execute_input":"2025-11-02T18:38:24.682749Z","iopub.status.idle":"2025-11-02T18:38:26.095010Z","shell.execute_reply.started":"2025-11-02T18:38:24.682722Z","shell.execute_reply":"2025-11-02T18:38:26.094378Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¸ Total images found: 486\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ------------------------------\n# 4. SPLIT DATA INTO TRAIN/VAL/TEST\n# ------------------------------\ntrain_df, temp_df = train_test_split(meta, test_size=0.3, stratify=meta['label'], random_state=RANDOM_SEED)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=RANDOM_SEED)\n\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n\n# Encode class labels as integers\nclass_names = sorted(train_df['label'].unique())\nlabel2idx = {c: i for i, c in enumerate(class_names)}\n\n# Save label mapping for Flask app\nwith open(os.path.join(WORK_DIR, \"class_indices.json\"), \"w\") as f:\n    json.dump({\"class_names\": class_names, \"label2idx\": label2idx}, f, indent=4)\nprint(\"âœ… Saved class mapping â†’ class_indices.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:38:34.366374Z","iopub.execute_input":"2025-11-02T18:38:34.366945Z","iopub.status.idle":"2025-11-02T18:38:34.392284Z","shell.execute_reply.started":"2025-11-02T18:38:34.366922Z","shell.execute_reply":"2025-11-02T18:38:34.389467Z"}},"outputs":[{"name":"stdout","text":"Train: 340, Val: 73, Test: 73\nâœ… Saved class mapping â†’ class_indices.json\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ------------------------------\n# 5. DATA AUGMENTATION PIPELINES\n# ------------------------------\ntrain_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, p=0.5),\n])\n\nval_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE)\n])\n\ndef _read_image(path):\n    \"\"\"\n    Reads and converts image to RGB.\n    \"\"\"\n    img = cv2.imread(path.decode('utf-8'))\n    if img is None:\n        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\ndef augment_image(path, label, transform):\n    \"\"\"\n    Applies augmentation (Albumentations) and scales image to [0,1].\n    \"\"\"\n    img = _read_image(path)\n    augmented = transform(image=img)\n    img = augmented['image'].astype('float32') / 255.0\n    return img, label\n\ndef tf_augment(path, label, train=True):\n    \"\"\"\n    Wraps Python image augmentations for TensorFlow Dataset.\n    \"\"\"\n    fn = lambda p, l: augment_image(p, l, train_transform if train else val_transform)\n    img, lbl = tf.numpy_function(fn, [path, label], [tf.float32, tf.int64])\n    img.set_shape((IMG_SIZE, IMG_SIZE, 3))\n    lbl.set_shape(())\n    return img, lbl\n\ndef df_to_dataset(df, batch_size=BATCH_SIZE, shuffle=True, train=True):\n    \"\"\"\n    Converts DataFrame into tf.data.Dataset with augmentations.\n    \"\"\"\n    paths = df['file_path'].astype(str).values\n    labels = df['label'].map(label2idx).astype(np.int64).values\n    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(paths), seed=RANDOM_SEED)\n    ds = ds.map(lambda p, l: tf_augment(p, l, train=train), num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return ds\n\n# Create datasets\ntrain_dataset = df_to_dataset(train_df, train=True)\nval_dataset = df_to_dataset(val_df, train=False)\ntest_dataset = df_to_dataset(test_df, train=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:38:43.437372Z","iopub.execute_input":"2025-11-02T18:38:43.437671Z","iopub.status.idle":"2025-11-02T18:38:45.295975Z","shell.execute_reply.started":"2025-11-02T18:38:43.437639Z","shell.execute_reply":"2025-11-02T18:38:45.295348Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\nI0000 00:00:1762108723.669914      37 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ------------------------------\n# 6. BASE CNN MODEL (Stage 1)\n# ------------------------------\ndef build_base_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), n_classes=len(class_names)):\n    \"\"\"\n    Simple CNN baseline for initial training.\n    \"\"\"\n    inputs = keras.Input(shape=input_shape)\n    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n    x = layers.MaxPool2D()(x)\n    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n    x = layers.MaxPool2D()(x)\n    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(n_classes, activation='softmax')(x)\n    return keras.Model(inputs, outputs)\n\nbase_model = build_base_model()\nbase_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\ncallbacks_base = [\n    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n    ModelCheckpoint(os.path.join(WORK_DIR, 'base_model.keras'), save_best_only=True, monitor='val_loss')\n]\n\nprint(\"\\nğŸš€ Training Base CNN...\\n\")\nhistory_base = base_model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS_BASE, callbacks=callbacks_base)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:38:53.099537Z","iopub.execute_input":"2025-11-02T18:38:53.100114Z","iopub.status.idle":"2025-11-02T18:39:10.264910Z","shell.execute_reply.started":"2025-11-02T18:38:53.100091Z","shell.execute_reply":"2025-11-02T18:39:10.264352Z"}},"outputs":[{"name":"stdout","text":"\nğŸš€ Training Base CNN...\n\nEpoch 1/8\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1762108736.103825     103 service.cc:148] XLA service 0x78aa94087840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1762108736.104648     103 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1762108736.463684     103 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 6/22\u001b[0m \u001b[32mâ”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2816 - loss: 1.3972","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1762108739.380979     103 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 194ms/step - accuracy: 0.2803 - loss: 1.3899 - val_accuracy: 0.2740 - val_loss: 1.3794\nEpoch 2/8\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.3240 - loss: 1.3714 - val_accuracy: 0.3014 - val_loss: 1.3640\nEpoch 3/8\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.2762 - loss: 1.3757 - val_accuracy: 0.2740 - val_loss: 1.3653\nEpoch 4/8\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.3284 - loss: 1.3679 - val_accuracy: 0.2877 - val_loss: 1.3498\nEpoch 5/8\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.3741 - loss: 1.3368 - val_accuracy: 0.4384 - val_loss: 1.2888\nEpoch 6/8\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.3996 - loss: 1.2814 - val_accuracy: 0.3425 - val_loss: 1.2807\nEpoch 7/8\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.4105 - loss: 1.2736 - val_accuracy: 0.5205 - val_loss: 1.2069\nEpoch 8/8\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.4147 - loss: 1.2284 - val_accuracy: 0.4521 - val_loss: 1.1545\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ------------------------------\n# 7. EFFICIENTNETB7 TRANSFER LEARNING (Stage 2)\n# ------------------------------\ndef build_efficientnet_model(n_classes=len(class_names)):\n    \"\"\"\n    Builds EfficientNetB7-based model for transfer learning.\n    \"\"\"\n    base = EfficientNetB7(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3), pooling='avg')\n    base.trainable = False  # Freeze base initially\n\n    inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    x = effnet_preprocess(inputs * 255.0)\n    x = base(x, training=False)\n    x = layers.Dropout(0.4)(x)\n    outputs = layers.Dense(n_classes, activation='softmax')(x)\n    return keras.Model(inputs, outputs)\n\neffnet_model = build_efficientnet_model()\neffnet_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\ncallbacks_effnet = [\n    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n    ModelCheckpoint(os.path.join(WORK_DIR, 'efficientnet_stage1.keras'), save_best_only=True, monitor='val_loss')\n]\n\nprint(\"\\nğŸŒ± Training EfficientNetB7 (Frozen base)...\\n\")\nhistory_effnet = effnet_model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS_TRANSFER, callbacks=callbacks_effnet)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:42:15.857695Z","iopub.execute_input":"2025-11-02T18:42:15.858294Z","iopub.status.idle":"2025-11-02T18:45:36.009358Z","shell.execute_reply.started":"2025-11-02T18:42:15.858267Z","shell.execute_reply":"2025-11-02T18:45:36.008717Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n\u001b[1m258076736/258076736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n\nğŸŒ± Training EfficientNetB7 (Frozen base)...\n\nEpoch 1/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 3s/step - accuracy: 0.3954 - loss: 1.3273 - val_accuracy: 0.7808 - val_loss: 0.8109\nEpoch 2/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 258ms/step - accuracy: 0.7572 - loss: 0.8269 - val_accuracy: 0.8493 - val_loss: 0.6081\nEpoch 3/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 300ms/step - accuracy: 0.7737 - loss: 0.6640 - val_accuracy: 0.8493 - val_loss: 0.5029\nEpoch 4/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 257ms/step - accuracy: 0.8744 - loss: 0.4944 - val_accuracy: 0.8630 - val_loss: 0.4463\nEpoch 5/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 258ms/step - accuracy: 0.8803 - loss: 0.4207 - val_accuracy: 0.8630 - val_loss: 0.4160\nEpoch 6/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 256ms/step - accuracy: 0.8807 - loss: 0.4434 - val_accuracy: 0.9041 - val_loss: 0.3807\nEpoch 7/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 257ms/step - accuracy: 0.9086 - loss: 0.3263 - val_accuracy: 0.8767 - val_loss: 0.3521\nEpoch 8/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 261ms/step - accuracy: 0.8979 - loss: 0.3630 - val_accuracy: 0.9178 - val_loss: 0.3368\nEpoch 9/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 256ms/step - accuracy: 0.9132 - loss: 0.3235 - val_accuracy: 0.9178 - val_loss: 0.3206\nEpoch 10/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 259ms/step - accuracy: 0.8953 - loss: 0.3156 - val_accuracy: 0.9178 - val_loss: 0.3114\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ------------------------------\n# 8. FINE-TUNING EFFICIENTNETB7\n# ------------------------------\n# Unfreeze base model for fine-tuning\neffnet_model.get_layer(index=1).trainable = True\neffnet_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\ncallbacks_finetune = [\n    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n    ModelCheckpoint(os.path.join(WORK_DIR, 'efficientnet_finetuned.keras'), save_best_only=True, monitor='val_loss')\n]\n\nprint(\"\\nğŸ”§ Fine-tuning EfficientNetB7...\\n\")\nhistory_finetune = effnet_model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS_TRANSFER, callbacks=callbacks_finetune)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:50:22.448391Z","iopub.execute_input":"2025-11-02T18:50:22.448705Z","iopub.status.idle":"2025-11-02T18:57:32.295894Z","shell.execute_reply.started":"2025-11-02T18:50:22.448684Z","shell.execute_reply":"2025-11-02T18:57:32.295274Z"}},"outputs":[{"name":"stdout","text":"\nğŸ”§ Fine-tuning EfficientNetB7...\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1762109639.412407     105 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109639.608624     105 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109639.998312     105 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109640.200974     105 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109640.701092     105 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109640.922190     105 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109641.576863     105 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109641.836309     105 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109642.368287     105 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109642.629347     105 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109643.599723     105 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109643.933002     105 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 372ms/step - accuracy: 0.6736 - loss: 0.8573","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1762109741.160307     106 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109741.359651     106 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109741.713771     106 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109741.932991     106 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109742.515294     106 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1762109742.774622     106 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 6s/step - accuracy: 0.6738 - loss: 0.8557 - val_accuracy: 0.9041 - val_loss: 0.3090\nEpoch 2/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 387ms/step - accuracy: 0.7340 - loss: 0.7549 - val_accuracy: 0.8904 - val_loss: 0.3169\nEpoch 3/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 384ms/step - accuracy: 0.7685 - loss: 0.6436 - val_accuracy: 0.8630 - val_loss: 0.3407\nEpoch 4/10\n\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 385ms/step - accuracy: 0.8382 - loss: 0.5285 - val_accuracy: 0.8356 - val_loss: 0.3610\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ------------------------------\n# 9. SAVE FINAL MODEL FOR FLASK\n# ------------------------------\nfinal_model_path = os.path.join(WORK_DIR, 'model_final_flask.h5')\neffnet_model.save(final_model_path)\nprint(\"âœ… Final model saved for Flask:\", final_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T19:00:54.939608Z","iopub.execute_input":"2025-11-02T19:00:54.940227Z","iopub.status.idle":"2025-11-02T19:00:57.943811Z","shell.execute_reply.started":"2025-11-02T19:00:54.940185Z","shell.execute_reply":"2025-11-02T19:00:57.943140Z"}},"outputs":[{"name":"stdout","text":"âœ… Final model saved for Flask: /kaggle/working/models/model_final_flask.h5\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ------------------------------\n# 10. EVALUATION\n# ------------------------------\ny_true, y_pred = [], []\nfor imgs, lbls in test_dataset:\n    preds = effnet_model.predict(imgs)\n    y_true.extend(lbls.numpy().tolist())\n    y_pred.extend(np.argmax(preds, axis=1).tolist())\n\nprint(\"\\nğŸ“Š Classification Report:\\n\")\nprint(classification_report(y_true, y_pred, target_names=class_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T19:01:29.018230Z","iopub.execute_input":"2025-11-02T19:01:29.019092Z","iopub.status.idle":"2025-11-02T19:02:06.848008Z","shell.execute_reply.started":"2025-11-02T19:01:29.019052Z","shell.execute_reply":"2025-11-02T19:02:06.847130Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17s/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20s/step\n\nğŸ“Š Classification Report:\n\n              precision    recall  f1-score   support\n\n       Guava       0.85      0.89      0.87        19\n       Lemon       1.00      0.89      0.94        19\n       Mango       0.82      0.93      0.87        15\n        Neem       1.00      0.95      0.97        20\n\n    accuracy                           0.92        73\n   macro avg       0.92      0.92      0.92        73\nweighted avg       0.92      0.92      0.92        73\n\n","output_type":"stream"}],"execution_count":16}]}